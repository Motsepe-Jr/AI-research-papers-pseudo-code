# AI Papers Pseudocode 

This repository contains pseudocode implementations for AI papers across different topics. The goal of this project is to provide pseudocode explanations and implementations for various AI techniques, algorithms, and methodologies.

## Table of Contents

- [Distributed Inference Papers](#distributed-inference-papers)
- [Finetune Papers](#finetune-papers)
- [Generative AI](#generative-ai)
- [Quantization Papers](#quantization-papers)
- [Sparse Papers](#sparse-papers)
- [Contributing](#contributing)
- [License](#license)

## Distributed Inference Papers

The `distributed_inference` directory contains pseudocode implementations for papers related to distributed inference techniques. This includes papers on distributed deep learning, model parallelism, and techniques for scaling up inference in distributed systems.

       -  Fast Distributed Inference Serving for Large Language Models

## Finetune Papers

The `finetune` directory houses pseudocode implementations for papers focusing on fine-tuning techniques in machine learning and deep learning. This includes papers on transfer learning, domain adaptation, and model adaptation techniques.

       - LoRA Finetuning Paper
       - Llama Adapters Paper

## Generative AI

In the `generative_ai` directory, you will find pseudocode implementations for papers related to generative AI models. This includes papers on generative adversarial networks (GANs), variational autoencoders (VAEs), and other generative models.

        - Toolformer


## Quantization Papers

The `quantization` directory contains pseudocode implementations for papers focused on model quantization techniques. This includes papers on quantization-aware training, post-training quantization, and other techniques for reducing model size and improving efficiency.

       - GPTQ Paper
       - Smooth Quant Paper

## Sparse Papers

In the `sparse` directory, you will find pseudocode implementations for papers related to sparsity techniques in AI models. This includes papers on sparse neural networks, pruning methods, and techniques for reducing computational complexity.

      - SparseGPT

## Contributing

Contributions to this project are welcome! If you have pseudocode implementations for AI papers in any of the mentioned categories or would like to contribute pseudocode for papers in other AI topics, feel free to submit a pull request. Please ensure that your additions include clear explanations, and relevant references to the original papers, and adhere to the existing code style.

## License

This project is licensed under the [MIT License](LICENSE). Feel free to use, modify, and distribute the pseudocode implementations for personal or commercial purposes.

