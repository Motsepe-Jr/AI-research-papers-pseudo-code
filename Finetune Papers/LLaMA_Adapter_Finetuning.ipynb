{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "                                                                             |              Prefix cross-attention\n",
        "                                                                             |\n",
        "  ┌─────────────────┐                                                        |               ┌──────────────────┐\n",
        "  ┆        x        ┆                                                        |               ┆      prefix      ┆\n",
        "  └─────────────────┘                                                        |               └──────────────────┘\n",
        "           |                                                                 |                        |\n",
        "           ▼                                                                 |                        ▼\n",
        "  ┌──────────────────┐                                                       |              ┌─────────────────────┐\n",
        "  ┆  self-attention  ┆ --------------------------------------------------------------┐      ┆  linear projection  ┆\n",
        "  └──────────────────┘                                                       |       ┆      └─────────────────────┘\n",
        "           |                                                                 |       ┆                |         \\\n",
        "           ▼                                                                 |       ▼                ▼          ▼\n",
        "         ╭───╮     ┌────────────────┐ ╭───╮ ┌──────────────────────────┐     |  ┌─────────┐    ┌──────────────┐  ┌────────────────┐\n",
        "         ┆ + ┆ ◀── ┆  gating factor ┆-┆ x ┆-┆  prefix cross-attention  ┆     |  ┆  query  ┆    ┆  prefix key  ┆  ┆  prefix value  ┆\n",
        "         ╰───╯     └────────────────┘ ╰───╯ └──────────────────────────┘     |  └─────────┘    └──────────────┘  └────────────────┘\n",
        "           |                                                                 |          \\             |           /\n",
        "           ▼                                                                 |           ▼            ▼          ▼\n",
        "                                                                             |         ┌────────────────────────────────┐\n",
        "                                                                             |         ┆  scaled dot-product attention  ┆\n",
        "                                                                             |         └────────────────────────────────┘\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "2jczUhr1xjQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References: \n",
        "         \n",
        "           LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n",
        "https://arxiv.org/abs/2303.16199\n",
        "\n",
        "               LightingAI Lit-Llama\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "KFtQR452yPst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "mFINWDM8hebi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2nRQf96DDZT"
      },
      "outputs": [],
      "source": [
        "class LLaMAAdapter(nn.Module):\n",
        "\n",
        "    def __init__(self, prompt_length, feature_dimension, num_layers, num_head):\n",
        "        super().__init__()\n",
        "\n",
        "        #  Learnable Adaption Prompts\n",
        "        self.prompt = nn.Embedding(prompt_length, feature_dimension)\n",
        "\n",
        "        # Zero Init Attention with Gating\n",
        "        self.gating_factors = nn.Parameter(torch.zeros(1, feature_dimension))\n",
        "\n",
        "        # k, q, v, projections\n",
        "        self.c_attn = nn.Linear(feature_dimension, 3 * feature_dimension, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(feature_dimension, 3 * feature_dimension , bias=False)\n",
        "\n",
        "    def forward(self, input_tokens, attention_mask):\n",
        "\n",
        "        q, k, v = self.c_attn(input_tokens).split(self.feature_dimensions, dim=2)\n",
        "\n",
        "        # Atention score for M + 1 (Word Tokens)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask, dropout_p=0.1)\n",
        "\n",
        "        _, prefix_key, prefix_value = self.c_attn(self.prompt).split(self.feature_dimensions, dim=2)\n",
        "\n",
        "        # Atention score for K (Adaption Prompt)\n",
        "        prefix_y = F.scaled_dot_product_attention(q, prefix_key, prefix_value, attn_mask=attention_mask, dropout_p=0.1)\n",
        "        \n",
        "        # Add a learnable gating factor, to adaptively control the importance prefix_y in the attention.\n",
        "        y = y + self.gating_factors * prefix_y\n",
        "        \n",
        "        y = self.c_proj(y)\n",
        "\n",
        "        return y "
      ]
    }
  ]
}